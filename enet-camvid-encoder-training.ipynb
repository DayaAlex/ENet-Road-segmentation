{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ENet - Real Time Semantic Segmentation.ipynb","provenance":[],"toc_visible":true,"version":"0.3.2"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7733151,"sourceType":"datasetVersion","datasetId":4518918}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ENet -  Real Time Semantic Segmentation for Road segmentation\n\n**Journal paper:**<br/>\nLink to the paper: https://arxiv.org/pdf/1606.02147.pdf <br/>\n**Code references**<br/>\npython implementation :https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation<br/>\nlua implemenattion by paper author : https://github.com/e-lab/ENet-training\n\nDaya Alex 5/4/24","metadata":{"colab_type":"text","id":"UhGVcnMbgr7y"}},{"cell_type":"markdown","source":"## Wandb login","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install the dependencies and Import them","metadata":{"colab_type":"text","id":"8qnyf_pzhKXv"}},{"cell_type":"code","source":"#!pip install pytorch-lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install segmentation_models_pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom torchvision import transforms\nimport math\nimport random\nfrom torchmetrics import JaccardIndex\nimport albumentations as A\nfrom torch.utils.data import DataLoader, Dataset\nfrom segmentation_models_pytorch.losses import DiceLoss\n\n\n#from torch.optim.lr_scheduler import StepLR\n#import torch.optim.lr_scheduler as lr_scheduler\nfrom PIL import Image\n\n","metadata":{"colab":{},"colab_type":"code","id":"5NCTHdEqj317","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the ENet model\n\nWe decided to to split the model to three sub classes:\n\n1) Initial block  \n\n2) RDDNeck - class for regular, downsampling and dilated bottlenecks\n\n3) ASNeck -  class for asymetric bottlenecks\n\n4) UBNeck - class for upsampling bottlenecks","metadata":{"colab_type":"text","id":"i26TZVXmhewY"}},{"cell_type":"code","source":"class InitialBlock(nn.Module):\n  \n  # Initial block of the model:\n  #         Input\n  #        /     \\\n  #       /       \\\n  # maxpool2d    conv2d-3x3\n  #       \\       /  \n  #        \\     /\n  #      concatenate\n  #          |\n  #         Batchnorm\n #        PReLU\n   \n    def __init__ (self,in_channels = 3,out_channels = 13):\n        super().__init__()\n\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, \n                                      stride = 2, \n                                      padding = 0)\n\n        self.conv = nn.Conv2d(in_channels, \n                                out_channels,\n                                kernel_size = 3,\n                                stride = 2, \n                                padding = 1)\n\n        self.prelu = nn.PReLU(16)\n\n        self.batchnorm = nn.BatchNorm2d(16)\n  \n    def forward(self, x):\n        \n        main = self.conv(x)\n        side = self.maxpool(x)\n        #print('main size ', main.size)\n        #print('side size ', side.size)\n        # concatenating on the channels axis\n        x = torch.cat((main, side), dim=1)\n        x = self.batchnorm(x)\n        x = self.prelu(x)\n        #print('init block size ',x.shape)\n        \n        return x","metadata":{"colab":{},"colab_type":"code","id":"kqHUezLfPBwn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RDDNeck(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n    \n  # Regular|Dilated|Downsampling bottlenecks:\n  #\n  #     Bottleneck Input\n  #        /        \\\n  #     Identity     \\  \n  #       /          \\\n  # maxpooling2d   conv2d-1x1(when downsamp flag is ON, otherwise 2x2)\n  # (when downsamp)    | BN +PReLU\n  # (-flag is ON)    conv2d-3x3\n  #      |             | BN +PReLU\n  #      |         conv2d-1x1\n  #      |             |\n  #  Padding2d     Regularizer(BN + dropout)\n  #(when i/p ch !=o/p ch) /   \n  #        \\            /\n  #      Summing + PReLU\n  #\n  # Params: \n  #  dilation (bool) - if True: creating dilation bottleneck\n  #  down_flag (bool) - if True: creating downsampling bottleneck\n  #  projection_ratio - ratio between input and output channels\n  #  relu - if True: relu used as the activation function else: Prelu us used\n  #  p - dropout ratio\n        \n        super().__init__()\n        \n        # Define class variables\n        self.in_channels = in_channels\n        self.reduced_depth = int(out_channels // projection_ratio)\n        self.out_channels = out_channels\n        self.dilation = dilation\n        self.down_flag = down_flag\n        \n        # calculating the number of reduced channels\n        if self.down_flag:\n            self.stride = 2\n            self.conv1_kernel = 2\n        else:\n            self.stride = 1\n            self.conv1_kernel = 1\n        \n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n        \n        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n                                      stride = 2,\n                                      padding = 0, return_indices=True)\n        \n        self.dropout = nn.Dropout2d(p=p)\n\n        self.prelu1 = activation\n        \n        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n                               out_channels = self.reduced_depth,\n                               kernel_size = self.conv1_kernel,\n                               stride = self.stride,\n                               padding = 0,\n                               bias = False,\n                               dilation = 1)\n        \n        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = 3,\n                                  stride = 1,\n                                  padding = self.dilation,\n                                  bias = True,\n                                  dilation = self.dilation)\n                                  \n        self.prelu2 = activation\n        \n        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.out_channels,\n                                  kernel_size = 1,\n                                  stride = 1,\n                                  padding = 0,\n                                  bias = False,\n                                  dilation = 1)\n        \n        self.prelu3 = activation\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n        self.identity = nn.Identity()\n        \n    def forward(self, x):\n        \n        bs = x.size()[0]\n        x_copy = self.identity(x)\n        \n        # Main\n        x = self.conv1(x)\n        #print(\" Conv1 called \")\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        #print(self.conv1_kernel, self.stride, x.shape)\n        \n        x = self.conv2(x)\n        #print(\" Conv2 called \")\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        #print(self.conv2.kernel_size, self.stride,self.dilation, x.shape)\n        \n        x = self.conv3(x)\n        #print(\" Conv3 called \")\n        x = self.batchnorm2(x)     \n        x = self.dropout(x)\n        #print(self.conv3.kernel_size, self.stride, x.shape)\n        \n        #other\n        if self.down_flag:\n            #print('downsampling flag is true')\n            x_copy, indices = self.maxpool(x_copy)\n            \n          \n        if self.in_channels != self.out_channels:\n            #print('input and output channels diffrence, so padding of side channel being carried out')\n            out_shape = self.out_channels - self.in_channels\n            #print('extra channels required ', out_shape)\n            \n            #padding and concatenating in order to match the channels axis of the side and main branches\n            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n            if torch.cuda.is_available():\n                extras = extras.cuda()\n            #elif torch.backends.mps.is_available():\n                #extras = extras.to('mps')\n            #print('x copy shape ',x_copy.shape)\n            #print('extras shape ', extras.shape)\n            x_copy = torch.cat((x_copy, extras), dim = 1)\n            #print('final side route shape ,', x_copy.shape)\n\n        # Summing main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        #print('final layer ', x.shape)\n        \n        if self.down_flag:\n            return x, indices\n        else:\n            return x","metadata":{"colab":{},"colab_type":"code","id":"r-nTIAS9bd9z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ASNeck(nn.Module):\n    def __init__(self, in_channels, out_channels, projection_ratio=4):\n      \n  # Asymetric bottleneck:\n  #\n  #     Bottleneck Input\n  #        /        \\\n  #       /          \\\n  #      |         conv2d-1x1\n  # Identity           | PReLU\n  #      |         conv2d-1x5\n  #      |             |\n  #      |         conv2d-5x1\n  #      |             | PReLU\n  #      |         conv2d-1x1\n  #      |             |\n  #       \\     Regularizer\n  #       \\           /  \n  #        \\         /\n  #      Summing + PReLU\n  #\n  # Params:    \n  #  projection_ratio - ratio between input and output channels\n        \n        super().__init__()\n        \n        # Define class variables\n        self.in_channels = in_channels\n        self.reduced_depth = int(out_channels // projection_ratio)\n        self.out_channels = out_channels\n        \n        self.dropout = nn.Dropout2d(p=0.1)\n        \n        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n                               out_channels = self.reduced_depth,\n                               kernel_size = 1,\n                               stride = 1,\n                               padding = 0,\n                               bias = False)\n        \n        self.prelu1 = nn.PReLU()\n        \n        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = (5, 1),\n                                  stride = 1,\n                                  padding = (2, 0),\n                                  bias = False)\n        \n        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth,\n                                  kernel_size = (1, 5),\n                                  stride = 1,\n                                  padding = (0, 2),\n                                  bias = True)############TRUE bias in original code###############\n        \n        self.prelu2 = nn.PReLU()\n        \n        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n                                  out_channels = self.out_channels,\n                                  kernel_size = 1,\n                                  stride = 1,\n                                  padding = 0,\n                                  bias = False)\n        \n        self.prelu3 = nn.PReLU()\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n        self.identity = nn.Identity()\n        \n    def forward(self, x):\n   \n        #main branch\n        x_copy = self.identity(x)\n        #print('side branch')\n        \n        # Side Branch\n        x = self.conv1(x)\n        #print('conv1 called')\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        #print(self.conv1.kernel_size,self.conv1.stride, x.shape)\n        \n        x = self.conv21(x)\n        #print('conv21 called')\n        #print(self.conv21.kernel_size, self.conv21.stride, x.shape)\n        x = self.conv22(x)\n        #print('conv22 called')\n        #print(self.conv22.kernel_size, self.conv22.stride, x.shape)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.conv3(x)\n        #print('conv3 called')   \n        x = self.dropout(x)\n        x = self.batchnorm2(x)\n        #print('final main ',self.conv3.kernel_size,self.conv3.stride, x.shape)\n\n        # Summing main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        #print('final total ', x.shape)\n        \n        return x","metadata":{"colab":{},"colab_type":"code","id":"tb_i1sCvtmMF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ENet_encoder(nn.Module):\n  \n  # Creating Enet model!\n  \n    def __init__(self, C):\n        super().__init__()\n        \n        # Define class variables\n        # C - number of classes\n        self.C = C\n        \n        # The initial block\n        self.init = InitialBlock()\n        \n        # The first bottleneck\n        self.b10 = RDDNeck(dilation=1, \n                           in_channels=16, \n                           out_channels=64, \n                           down_flag=True, \n                           p=0.01)\n        \n        self.b11 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        self.b12 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        self.b13 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        self.b14 = RDDNeck(dilation=1,\n                           in_channels=64, \n                           out_channels=64, \n                           down_flag=False, \n                           p=0.01)\n        \n        \n        # The second bottleneck\n        self.b20 = RDDNeck(dilation=1, \n                           in_channels=64, \n                           out_channels=128, \n                           down_flag=True)\n        \n        self.b21 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b22 = RDDNeck(dilation=2, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b23 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b24 = RDDNeck(dilation=4, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b25 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b26 = RDDNeck(dilation=8, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b27 = ASNeck(in_channels=128, \n                          out_channels=128)\n        self.b28 = RDDNeck(dilation=16, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        \n        # The third bottleneck\n        self.b31 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b32 = RDDNeck(dilation=2, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b33 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b34 = RDDNeck(dilation=4, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b35 = RDDNeck(dilation=1, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b36 = RDDNeck(dilation=8, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        self.b37 = ASNeck(in_channels=128, \n                          out_channels=128)\n        \n        self.b38 = RDDNeck(dilation=16, \n                           in_channels=128, \n                           out_channels=128, \n                           down_flag=False)\n        \n        #fully convolutional layer to get the encoder output\n        self.enc_conv = nn.Conv2d(in_channels=128,\n                                 out_channels = 12,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0,\n                                 bias =False)\n    def forward(self, x):\n        \n        # The initial block\n        x = self.init(x)\n        #print('^^^^^^^^^^^^^^init block^^^^^^^^^^^^^^^^')\n        \n        # The first bottleneck\n        x, i1 = self.b10(x)\n        #print('................bottleneck_10 complete................')\n        x = self.b11(x)\n        #print('................bottleneck_11 complete...........')\n        x = self.b12(x)\n        #print('...............bottleneck_12 complete............')\n        x = self.b13(x)\n        #print('............bottleneck_13 complete............')\n        x = self.b14(x)\n        #print('.................bottleneck_14 complete.............')\n        \n        # The second bottleneck\n        x, i2 = self.b20(x)\n        #print('----------------bottleneck_20 complete-----------')\n        x = self.b21(x)\n        #print('----------------bottleneck_21 complete-----------')\n        x = self.b22(x)\n        #print('-----------bottleneck_22 complete--------------')\n        x = self.b23(x)\n        #print('-----------bottleneck_23 complete---------------')\n        x = self.b24(x)\n        #print('--------------bottleneck_24 complete------------')\n        x = self.b25(x)\n        #print('-------------bottleneck_25 complete------------')\n        x = self.b26(x)\n        #print('-------------bottleneck_26 complete-------------')\n        x = self.b27(x)\n        #print('-------------bottleneck_27 complete-------------')\n        x = self.b28(x)\n        #print('------------bottleneck_28 complete----------------')\n        \n        # The third bottleneck\n        x = self.b31(x)\n        #print('********bottleneck_31 complete************')\n        x = self.b32(x)\n        #print('********bottleneck_32 complete************')\n        x = self.b33(x)\n        #print('********bottleneck_33 complete************')\n        x = self.b34(x)\n        #print('********bottleneck_34 complete************')\n        x = self.b35(x)\n        #print('********bottleneck_35 complete************')\n        x = self.b36(x)\n        #print('********bottleneck_36 complete************')\n        x = self.b37(x)\n        #print('********bottleneck_37 complete************')\n        x = self.b38(x)\n        #print('********bottleneck_38 complete************')\n        \n\n        x = self.enc_conv(x)\n      \n        \n        return x, i1,i2\n        ","metadata":{"colab":{},"colab_type":"code","id":"Z1pz_bve690y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiate the ENet model","metadata":{"colab_type":"text","id":"Jg1Rnb3uhnxR"}},{"cell_type":"markdown","source":"Move the model to cuda if available","metadata":{"colab_type":"text","id":"W5kaK6CnhwG_"}},{"cell_type":"code","source":"# Checking if there is any gpu available and pass the model to gpu or cpu\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nencoder = ENet_encoder(12)\nencoder.to(device)","metadata":{"colab":{},"colab_type":"code","id":"lZcgE-F_hvxX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the class weights (only for cross-entropy loss)","metadata":{"colab_type":"text","id":"_4qD8VBah2K2"}},{"cell_type":"code","source":"class_weights = [ 6.09752836 , 4.50045501 ,31.41213452 , 3.44543644 ,14.96523731 , 8.93200049,\n 29.91471566 ,30.77730888 ,12.69374868 ,37.01958742 ,41.13626664 ,18.56930107]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## visualising pipe","metadata":{}},{"cell_type":"code","source":"def verify_dataset_normalization(loader,h=360,w=480):\n    \"\"\"finds mean and std dev for whole data batch\n    \"\"\"\n    # Initialize accumulators\n    channel_sum = torch.tensor([0.0, 0.0, 0.0])\n    channel_squared_sum = torch.tensor([0.0, 0.0, 0.0])\n    num_batches = 0\n\n    for images, _ in train_loader:\n        #print(images.shape)\n        # Accumulate sum and squared sum for each channel\n        channel_sum += images.sum(dim=[0, 2, 3])\n        channel_squared_sum += (images ** 2).sum(dim=[0, 2, 3])\n        num_batches += images.shape[0]\n\n    # Calculate mean and standard deviation\n    mean = channel_sum / (num_batches * h * w)\n    std = (channel_squared_sum / (num_batches * h * w) - mean ** 2) ** 0.5\n\n    return mean, std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augmentation(mode='train', h= 360, w =480):\n    \"\"\" resizes image to input size and mask to a downsampled size, \n        applies horizontal flip and color jitter augmentation only to trainsets\n        \n    \"\"\"\n    if mode == 'train':\n        img_transformation = A.Compose([\n                        A.Resize(h,w),\n                        A.HorizontalFlip(p= 0.5),\n                        #A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n\n                    ])\n        #mask size different from image size\n        mask_transformation = A.Compose([\n                    A.Resize(h//8, w//8),\n                    A.HorizontalFlip(p=0.5)\n                    ])\n        \n    else:\n        img_transformation =A.Resize(h, w)\n        mask_transformation = A.Resize(h//8, w//8)\n        \n    return img_transformation,  mask_transformation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#img = cv2.imread('/kaggle/input/CamVid/train/', cv2.COLOR_BGR2RGB)\n#mask = cv2.imread()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CamvidDataset(Dataset):\n    \"\"\"custom camvid datset that returns images and the corresponding masks after augmentation and normalisation  \n        \n    \"\"\"\n    def __init__(self, img_path, mask_path, augmentation, norm_transform=True, road_idx=None):\n        self.filenames_t = os.listdir(img_path)\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.norm_transform = norm_transform\n        self.augmentation = augmentation\n        #for exttracting road mask\n        #self.road_idx = road_idx \n\n    def __len__(self):\n        return len(self.filenames_t)\n\n    def __getitem__(self, idx):\n        each_img_path = os.path.join(self.img_path, self.filenames_t[idx])\n        each_mask_path = os.path.join(self.mask_path, self.filenames_t[idx])\n        img = cv2.imread(each_img_path, cv2.COLOR_BGR2RGB)\n        label_array = cv2.imread(each_mask_path, cv2.IMREAD_GRAYSCALE)#h,w array datatype\n        label = np.expand_dims(label_array, axis = -1)#h,w,c\n        \n        if self.augmentation:\n            img_transforms, mask_transforms = self.augmentation\n            seed = 7\n            random.seed(seed)\n            img = img_transforms(image=img)['image']# albumentations must be passed with named argument, and gets stored with that name as key\n            random.seed(seed)\n            mask = mask_transforms(image=label)['image']\n            \n        if self.norm_transform:\n            normalize_tensor = transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.4119, 0.4251, 0.4327], \n                                                [0.3047, 0.3096, 0.3054])\n                                     ])\n            img = normalize_tensor(img)\n            \n        if not isinstance(mask, torch.Tensor):\n            mask = torch.tensor(mask, dtype=torch.uint8)\n        mask = mask.permute(2, 0, 1)#c,h,w\n        #print(mask.shape)\n        mask = mask.squeeze()#h,w\n        #print(mask.shape)\n\n        return img, mask\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Idd7ClassesDataset(Dataset):\n    def __init__(self, img_path, mask_path, augmentation, norm_transform=True, road_idx=None):\n        self.filenames_t = glob.glob(training_path+f'*\\*.jpg')\n        self.filenames_s = glob.glob(segmented_path+f'*\\*[0-9]_label.png')\n        self.norm_transform = norm_transform\n        self.dual_transform = dual_transform\n        self.road_idx = road_idx\n\n    def __len__(self):\n        return len(self.filenames_t)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames_t[idx]\n        mask_path = self.filenames_s[idx]\n        img = cv2.imread(each_img_path, cv2.COLOR_BGR2RGB)\n        label_array = cv2.imread(each_mask_path, cv2.IMREAD_GRAYSCALE)#h,w array datatype\n        label = np.expand_dims(label_array, axis = -1)#h,w,c\n        \n        if self.augmentation:\n            img_transforms, mask_transforms = self.augmentation\n            seed = 7\n            random.seed(seed)\n            img = img_transforms(image=img)['image']# albumentations must be passed with named argument, and gets stored with that name as key\n            random.seed(seed)\n            mask = mask_transforms(image=label)['image']\n            \n        if self.norm_transform:\n            normalize_tensor = transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.3576, 0.3713, 0.3657], \n                                                [0.2608, 0.2723, 0.2943])\n                                     ])\n            img = normalize_tensor(img)\n            \n        if not isinstance(mask, torch.Tensor):\n            mask = torch.tensor(mask, dtype=torch.uint8)\n        mask = mask.permute(2, 0, 1)#c,h,w\n        #print(mask.shape)\n        mask = mask.squeeze()#h,w\n        #print(mask.shape)\n\n        return img, mask\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = CamvidDataset('/kaggle/input/CamVid/train/','/kaggle/input/CamVid/trainannot/',augmentation('train'))\n# val_dataset = CamvidDataset('/kaggle/input/CamVid/val/','/kaggle/input/CamVid/valannot/',augmentation('val'))\n# test_dataset = CamvidDataset('/kaggle/input/CamVid/test/','/kaggle/input/CamVid/testannot/',augmentation('test'))\n\ntrain_dataset = Idd7ClassesDataset(\"/kaggle/input/idd-lite1-6k/idd20k_lite/leftImg8bit/train/\",\n                                    \"/kaggle/input/idd-lite1-6k/idd20k_lite/gtFine/train/\", augmentation('train'))\nval_dataset = Idd7ClassesDataset(\"/kaggle/input/idd-lite1-6k/idd20k_lite/leftImg8bit/val/\",\n                                    \"/kaggle/input/idd-lite1-6k/idd20k_lite/gtFine/val/\",augmentation('val'))\n\n# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n# test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False) \n    \n# mean_train, std_train = verify_dataset_normalization(train_loader)\n# mean_val, std_val = verify_dataset_normalization(val_dataset)\n# mean_test, std_test = verify_dataset_normalization(test_loader)\n\n# print(f'mean and std dev of train set is {mean_train}, {std_train}')\n# print(f'mean and std dev of val set is {mean_val}, {std_val}')\n# print(f'mean and std dev of test set is {mean_test}, {std_test}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_segmap(image, threshold=0.5):\n    \n    #print(image)#RGB\n    Sky = [0, 0, 0]\n    Building = [0, 0, 153]\n    Pole = [0, 0, 255]\n    Road = [51, 153, 255]\n    Pavement = [0, 255, 255]\n    Tree = [128, 255, 0]\n    SignSymbol = [255, 255, 0]\n    Fence = [64, 64, 128]\n    Car = [255, 128, 0]\n    Pedestrian = [255, 0, 127]\n    Bicyclist = [255, 204, 255]\n    Background_scene = [255,255,255]\n\n    label_colours = np.array([Sky, Building, Pole, Road, \n                              Pavement, Tree, SignSymbol, Fence, Car, \n                              Pedestrian, Bicyclist, Background_scene]).astype(np.uint8)\n    \n    print(label_colours.shape)\n    r = np.zeros_like(image).astype(np.uint8)\n    g = np.zeros_like(image).astype(np.uint8)\n    b = np.zeros_like(image).astype(np.uint8)\n    \n    for l in range(0, 12):\n        r[image == l] = label_colours[l, 0]\n        g[image == l] = label_colours[l, 1]\n        b[image == l] = label_colours[l, 2]\n\n    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n    rgb[:, :, 0] = r\n    rgb[:, :, 1] = g\n    rgb[:, :, 2] = b\n    return rgb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_sample(dataset, idx):\n    img, mask = dataset[idx]  # Fetch the image and mask using the dataset's __getitem__ method\n    fig, ax = plt.subplots(1, 2)\n    ax[0].imshow(img.permute(1, 2, 0))  # Assuming img is a PyTorch tensor of shape [C, H, W]\n    ax[0].set_title(\"Image\")\n    ax[1].imshow(mask, cmap='gray')  # Assuming mask is a PyTorch tensor of shape [C, H, W] and C=1 for grayscale\n    ax[1].set_title(\"Mask\")\n    plt.show()\n\n# Example usage\nvisualize_sample(train_dataset, 5)  # Visualize the first sample in the train dataset\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 0\n\nimg, mask = train_dataset[idx]\nprint(img.shape, mask.shape)\n#print(mask)\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n        \nax1.set_title('IMAGE')\nax1.imshow(img.permute(1,2,0).squeeze())\n        \nax2.set_title('GROUND TRUTH')\nax2.imshow(mask.squeeze(),cmap = 'gray')\nmask_segmap =decode_segmap(mask.squeeze().numpy())\n\nax2.imshow(mask_segmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Hyperparameters","metadata":{"colab_type":"text","id":"wwnNuFIfhsXm"}},{"cell_type":"code","source":"from torchmetrics.classification import MulticlassJaccardIndex\njaccard = MulticlassJaccardIndex(num_classes=12).to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop","metadata":{"colab_type":"text","id":"CFIdlVWviBYl"}},{"cell_type":"code","source":"\n# bc_train = 367 // batch_size # count of mini_batch train\n# bc_eval = 101 // batch_size  # count of mini_batch validation\n# bc_test = 233 // batch_size\n\n# epochs = 20\n# save_every = 200\ntotal_run :int = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_image_table(X_batch, outputs, mask_batch):\n    table = wandb.Table(columns = [\"images\", \"predictions\", \"targets\"] \n            )\n    \n    for X_img, output, mask in zip(X_batch.to(\"cpu\"), outputs.to(\"cpu\"), mask_batch.to(\"cpu\")):\n        segmap_pred = decode_segmap(output.data.max(0)[1].numpy())#pass the max prob channel\n        segmap_gt = decode_segmap(mask.numpy())\n        \n        table.add_data(wandb.Image(X_img.numpy().transpose(1,2,0)*255), \n                  wandb.Image(segmap_pred), \n                  wandb.Image(segmap_gt)\n                 )\n    wandb.log({\"predictions_table\": table}, commit = False) #lets commit to the server with the metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_loop(model, val_loader, criterion, log_images = False, batch_idx = 0):\n    \"\"\" params: model, val_loader,loss function, \n        flag log_images: to decide to log the predictions in table\n        flag batch_idx : to log a single batch consistently over all epochs, default 0th batch, \n        \n        returns validation loss and accuracy averaged for that epoch \"\"\"\n    model.eval()\n    val_loss = 0\n    \n    with torch.inference_mode():\n        cum_jacc = 0\n        for step, (X_batch, mask_batch)in enumerate(val_loader):\n            \n            X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n            outputs,_,_ = model(X_batch.float())\n            val_loss += criterion(outputs, mask_batch.long())* X_batch.size(0)\n            cum_jacc += jaccard(outputs, mask_batch)\n            \n            if step == batch_idx and log_images:\n                log_image_table(X_batch, outputs, mask_batch)\n        return val_loss/len(val_loader.dataset), cum_jacc/len(val_loader.dataset)\n            \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for run in range(total_run):\n    config ={\"learning_rate\": 5e-4,\n                        \"batch_size\": 10,\n                        \"epochs\": 100,\n                       } \n    wandb.init(project ='enet_idd_lite_encoder_training',\n                name=f'lr={config[\"learning_rate\"]}_bs={config[\"batch_size\"]}_epochs={config[\"epochs\"]}',\n               config=config)\n\n    config = wandb.config\n    \n    train_loader = DataLoader(train_dataset, config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, config.batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, config.batch_size, shuffle=False) \n    \n    n_steps_per_epoch = math.ceil(len(train_dataset)/config.batch_size)\n    model = encoder\n    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n    optimizer = torch.optim.Adam(encoder.parameters(), \n                             lr=config.learning_rate,\n                             weight_decay=2e-4)\n    \n    \n    model.to(device)\n    element_step_ct = 0\n    step_ct = 0\n    for e in range(config.epochs):\n          \n        model.train()\n    \n        for step, (X_batch,mask_batch) in enumerate(train_loader):\n            \n            # assign data to cpu/gpu\n            X_batch, mask_batch = X_batch.to(device), mask_batch.to(device) \n            out,i1,i2 = model(X_batch.float())\n\n            optimizer.zero_grad()\n            train_loss = criterion(out, mask_batch.long())\n            \n            # update weights\n            train_loss.backward()\n            optimizer.step()\n            element_step_ct += len(X_batch)\n            \n            train_metrics = {\"train/loss\": train_loss,#loss per batch\n                       \"train/epoch\": (step +1 +(n_steps_per_epoch*e))/n_steps_per_epoch,\n                       \"train:element_step_ct\":element_step_ct\n                      }\n            if step+1<n_steps_per_epoch:\n                wandb.log(train_metrics)\n            \n            step_ct +=1\n        \n        val_loss, jacc_acc = validation_loop(model, val_loader, criterion, log_images = (e == (config.epochs-1))) \n        val_metrics = {\"val/val_loss\": val_loss,\n                      \"val/val_accuracy\": jacc_acc}\n        \n        ##logging val metrics alongside train metrics\n        wandb.log({**train_metrics, **val_metrics })\n        \n        #print final achieved losses and accuracy at the end of each epoch\n        print(f\"Train loss: {train_loss:.3f},Valid_loss: {val_loss:.3f},Accuracy: {jacc_acc:.2f}\")\n        \n        if e == (config.epochs-1):\n            checkpoint = {\n            'epochs' : e,\n            'state_dict' : model.state_dict(),\n            'maxpool_indices_1':i1,\n            'maxpool_indices_2':i2,\n            'jacc':jacc_acc,\n            }\n            torch.save(checkpoint, './encoder idd {}th epoch, indices with state_dic.pth'.format(e))\n            print ('Model is saved!')\n            \nwandb.finish()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"colab_type":"code","id":"WQ6XJzl6Ta1_","outputId":"a3f62522-391d-4e05-f138-11c78a0d90cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################# VALID LOADER ##############################\nimg_aug_func, mask_aug_func = Augmentation('val')\n\ndual_transform = DualTransform(img_aug_func, mask_aug_func)\n\ntransform_norm = None\n#transforms.Compose([\n    #transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.4119, 0.4251, 0.4327], std=[0.3047, 0.3097, 0.3054])\n#])\n\n# Instantiate your dataset\ntest_dataset = CustomDataset('/kaggle/input/CamVid/test/',\n                               '/kaggle/input/CamVid/testannot/',3 , \n                               norm_transform=transform_norm, dual_transform=dual_transform)\n\n# Create your data loader\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n\n# Now you can iterate over train_loader to calculate mean and standard deviation\nmean_test, std_test = verify_dataset_normalization(test_loader)\nprint('mean and std dev of test set :', mean_test,std_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef test_model(model, epochs,test_loaders, criterion, optimizer,device):\n    jacc_t_acc =[]   \n    test_losses =[]\n    for e in range(1,epochs+1):\n        model.to(device)\n        with torch.no_grad():\n            model.eval()\n            \n            test_loss = 0\n            cum_test_iou =0\n            cum_jacc_t = 0\n            # Validation loop\n            for inputs, labels in tqdm(val_loader):\n                \n                inputs, labels = inputs.to(device), labels.squeeze(1).to(device)\n                    \n                out, id1,id2 = model(inputs)\n                loss = criterion(out, labels.long())\n                test_loss += loss.item()\n                preds_ohe = torch.argmax(out, dim=1)\n                  \n                #cum_test_iou += calculate_mIoU(preds_ohe, labels, 2) \n                target = labels.squeeze(1)         \n                cum_jacc_t += jaccard(preds_ohe,target)\n\n        test_losses.append(test_loss/bc_test)\n        #test_acc.append(cum_test_iou/batch_size)\n        jacc_t_acc.append(cum_jacc_t/bc_test)\n        print ('Epoch {}/{}...'.format(e, epochs),\n                \n                'test_Loss {:6f}'.format(test_losses[-1]),\n                #'test_acc {:6f}'.format(test_acc[-1]),\n                'jacc_t_acc {:6f}'.format(jacc_t_acc[-1]))\n        \n    print ('Epoch {}/{}...'.format(e, epochs),\n           'Total Mean test loss:{:6f}'.format(sum(test_losses)/epochs))\n    return  test_losses, jacc_t_acc\n      \n      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_losses, test_acc = test_model(encoder, epochs,test_loader, criterion, optimizer, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Move the test_losses tensor to the CPU before converting to NumPy array\nplt.plot(train_losses, label = 'train_losses')\nplt.plot(eval_losses, label='valid_losses')\nplt.plot(test_losses, label='test_losses')\nplt.title('Valid loss and test loss for Cross-Entropy')\nplt.xlabel('epochs')\nplt.ylabel(' CE')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(eval_acc)\nprint(test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to ensure all elements are tensors and on the CPU\ndef prepare_for_plotting(data_list):\n    processed_list = []\n    for item in data_list:\n        # Check if the item is a tensor\n        if isinstance(item, torch.Tensor):\n            # Move tensor to CPU and convert to numpy, then to scalar\n            processed_item = item.cpu().item()\n        else:\n            # Item is already a scalar\n            processed_item = item\n        processed_list.append(processed_item)\n    return processed_list\n\n# Prepare eval_acc and test_acc for plotting\neval_acc_plot = prepare_for_plotting(eval_acc)\ntest_acc_plot = prepare_for_plotting(test_acc)\n\n# Now you can plot\nplt.plot(eval_acc_plot, label='val_accuracy')\nplt.plot(test_acc_plot, label='test_accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('IoU')\nplt.legend(frameon=True)\nplt.title('Validation and Test Accuracy trend combined')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( test_losses,label='test loss')\nplt.plot( test_acc_plot,label='test acc')\nplt.legend(frameon=True)\nplt.title('test acc and test loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( eval_losses,label='val loss')\nplt.plot( eval_acc_plot,label='val acc')\nplt.legend(frameon=True)\nplt.title('val acc and val loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Infer using the trained model","metadata":{"colab_type":"text","id":"bdp5YLb0ibFO"}},{"cell_type":"code","source":"h = 360\nw = 480\n\nimage_path = '/kaggle/input/CamVid/test/0001TP_008730.png'\n# Assuming the dataset is camvid\n\ntmg_ = plt.imread(image_path)\ntmg_ = cv2.resize(tmg_, (h, w), cv2.INTER_NEAREST)\ntmg = torch.tensor(tmg_).unsqueeze(0).float()\ntmg = tmg.transpose(2, 3).transpose(1, 2)\ntmg = tmg.to(device)\n\nwith torch.no_grad():\n    out1, id1, id2 = encoder(tmg.float())\n    out1 = out1.squeeze(0)\nb_ = out1.data.max(0)[1].cpu().numpy()\n\ndecoded_segmap = decode_segmap(b_)\n\ngt_path = image_path.replace('test','testannot') \ngt = plt.imread(gt_path)\ngt = cv2.resize(gt, (h, w), cv2.INTER_NEAREST)\n\nimages = {\n 0 : ['Input Image', tmg_],\n 1 : ['Predicted Segmentation', decoded_segmap],\n 2 : ['ground truth', gt]\n}\n\nplt.title('cross_entropy_multi-class_80_epochs')\n\nshow_images(images)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = {\n            'epochs' : epochs,\n            'state_dict' : encoder.state_dict(),\n            'train_error' :train_losses,\n            'val_error' : eval_losses,\n            'iou_trend' : eval_acc,\n            'optimizer': optimizer.state_dict()\n            #'scheduler': scheduler.state_dict()\n            \n            }\ntorch.save(checkpoint, '/kaggle/working/ckpt-enet-camvid_CE-{}-.pth'.format(100))\nprint ('Model saved!')\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/working/ckpt-enet-camvid-ce-12-100-21.237043470144272.pth')\nenet = ENet(12)\nenet = enet.to(device)\nstate_dict = checkpoint['.squeeze()']\nenet.load_state_dict(state_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Manual calculation of mIoU","metadata":{}},{"cell_type":"code","source":"def intersection_over_union(pred_mask, true_mask):\n    intersection = torch.logical_and(pred_mask, true_mask).sum().item()\n    union = torch.logical_or(pred_mask, true_mask).sum().item()\n\n    iou = intersection / union if union > 0 else 0.0\n    return iou\n\ndef calculate_mIoU(predictions, targets, num_classes):\n    class_iou = [0] * num_classes\n    \n    for class_idx in range(num_classes):\n        pred_mask = (predictions == class_idx)\n        true_mask = (targets == class_idx)\n        class_iou[class_idx] = intersection_over_union(pred_mask, true_mask)\n\n    mean_iou_value = sum(class_iou) / num_classes\n    \n    #print('road iou',class_iou[4])\n    return mean_iou_value\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
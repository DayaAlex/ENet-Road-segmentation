



100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 367/367 [00:06<00:00, 52.44it/s]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

 88%|███████████████████████████████████████████████████████████████████████████████████████████▌            | 323/367 [00:03<00:00, 91.23it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 367/367 [00:04<00:00, 86.83it/s]
/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
   | Name     | Type          | Params
--------------------------------------------
0  | loss     | loss_function | 0
1  | init     | InitialBlock  | 412
2  | b10      | DownRDDNeck   | 4.5 K
3  | b11      | RDDNeck       | 4.5 K
4  | b12      | RDDNeck       | 4.5 K
5  | b13      | RDDNeck       | 4.5 K
6  | b14      | RDDNeck       | 4.5 K
7  | b20      | DownRDDNeck   | 21.9 K
8  | b21      | RDDNeck       | 17.8 K
9  | b22      | RDDNeck       | 17.8 K
10 | b23      | ASNeck        | 18.8 K
11 | b24      | RDDNeck       | 17.8 K
12 | b25      | RDDNeck       | 17.8 K
13 | b26      | RDDNeck       | 17.8 K
14 | b27      | ASNeck        | 18.8 K
15 | b28      | RDDNeck       | 17.8 K
16 | b31      | RDDNeck       | 17.8 K
17 | b32      | RDDNeck       | 17.8 K
18 | b33      | ASNeck        | 18.8 K
19 | b34      | RDDNeck       | 17.8 K
20 | b35      | RDDNeck       | 17.8 K
21 | b36      | RDDNeck       | 17.8 K
22 | b37      | ASNeck        | 18.8 K
23 | b38      | RDDNeck       | 17.8 K
24 | enc_conv | Conv2d        | 1.5 K
--------------------------------------------
334 K     Trainable params
0         Non-trainable params
334 K     Total params
1.339     Total estimated model params size (MB)
/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/ENet-Road-segmentation/train.py", line 43, in <module>
    trainer.fit(model, datamod)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/teamspace/studios/this_studio/ENet-Road-segmentation/model.py", line 666, in validation_step
    self.val_step_outputs.append(torch.softmax(out), dim =1)
TypeError: softmax() received an invalid combination of arguments - got (Tensor), but expected one of:
 * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)
 * (Tensor input, name dim, *, torch.dtype dtype)